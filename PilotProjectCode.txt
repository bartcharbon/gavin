package org.molgenis.data.annotation.utils;

import java.io.File;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Scanner;

import org.apache.commons.math3.stat.descriptive.moment.Mean;
import org.apache.commons.math3.stat.inference.MannWhitneyUTest;
import org.molgenis.data.AttributeMetaData;
import org.molgenis.data.Entity;
import org.molgenis.data.annotation.RepositoryAnnotator;
import org.molgenis.data.annotation.entity.impl.CaddAnnotator;
import org.molgenis.data.annotation.entity.impl.ExacAnnotator;
import org.molgenis.data.annotation.entity.impl.SnpEffServiceAnnotator;
import org.molgenis.data.annotator.tabix.TabixReader;
import org.molgenis.data.support.DefaultAttributeMetaData;
import org.molgenis.data.support.DefaultEntityMetaData;
import org.molgenis.data.vcf.VcfRepository;
import org.molgenis.framework.server.MolgenisSettings;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.ApplicationContext;
import org.springframework.context.annotation.AnnotationConfigApplicationContext;
import org.springframework.stereotype.Component;

/**
 * 
 * Trying out some ideas
 *
 */
@Component
public class MapCADDTL
{

	// "CADD-TL mapping"
	// test 3 inheritance models: recessive, additive, dominant
	// test 3 frequency models: singleton (0%), rare (<1%), common (<5%)
	//
	// simple mechanics:
	// 1. get set of candidate variants (+genotypes) from patients by filtering on MAF, impact, etc
	// 2. from within the same sequence feature (ie. gene), get comparable variants from the population
	// meaning: same filter criteria. get many, but not more than necessary while staying close to patient variants
	// ---> QUESTION: okay if the same, with different genotypes???
	// 3. get CADD scores and multiply by inheritance model
	// (homalt always x2, but rec: 0x het, add: 1x het, dom: 2x het)
	// 4. test and plot!
	//
	// CURRENT LIMITATIONS:
	// - Exome only, later on use 1000G+GoNL data for whole-genome scan
	// - Ignoring multi-allelic variants for the moment as they are tricky to parse
	// - For developing, only use subset
	// - handle hemizygous on X / Y ??
	// - see TODO
	// - see ftp://ftp.broadinstitute.org/pub/ExAC_release/release0.3/README.known_issues
	//

	@Autowired
	private MolgenisSettings molgenisSettings;

	@Autowired
	private ApplicationContext applicationContext;

	public static void main(String[] args) throws Exception
	{
		// See http://stackoverflow.com/questions/4787719/spring-console-application-configured-using-annotations
		AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext("org.molgenis.data.annotation");
		MapCADDTL main = ctx.getBean(MapCADDTL.class);
		main.run(args);
	}

	public void run(String[] args) throws Exception
	{

		if (args.length != 5)
		{
			throw new Exception("Must supply 5 arguments");
		}

		double MAF = 0.01;
		String inheritance = "additive";

		File patientVcfFile = new File(args[0]);
		File patientSampleIds = new File(args[1]); // TODO nicer ???
		File exacFileLocation = new File(args[2]);
		File caddFileLocation = new File(args[3]);
		File outputFileLocation = new File(args[4]);

		// load the sample identifiers of the patients within the VCF file
		// (we are not interested non-disease samples such as parents of trios etc)
		Scanner s = new Scanner(patientSampleIds);
		ArrayList<String> sampleIds = new ArrayList<String>();
		while (s.hasNextLine())
		{
			sampleIds.add(s.nextLine());
		}
		s.close();

		VcfRepository vcfRepo = new VcfRepository(patientVcfFile, this.getClass().getName());

		molgenisSettings.setProperty(ExacAnnotator.EXAC_FILE_LOCATION_PROPERTY, exacFileLocation.getAbsolutePath());
		molgenisSettings.setProperty(CaddAnnotator.CADD_FILE_LOCATION_PROPERTY, caddFileLocation.getAbsolutePath());

		HashMap<String, ArrayList<Entity>> patientExomeCandidateVariants = getPatientExomeCandidateVariants(vcfRepo,
				MAF);
		HashMap<String, ArrayList<Double>> patientCADDscores = calculatePatientGenotypeCADDscores(
				patientExomeCandidateVariants, sampleIds, inheritance);

		HashMap<String, ArrayList<String>> populationExomeCandidateVariants = getPopulationExomeCandidateVariants(
				patientExomeCandidateVariants, exacFileLocation, MAF);
		HashMap<String, ArrayList<Double>> populationCADDscores = calculatePopulationGenotypeCADDscores(
				populationExomeCandidateVariants, inheritance);
		
		
		System.out.println("So! we can now compare....");
		
		PrintWriter pw = new PrintWriter(outputFileLocation, "UTF-8");
		
		for(String sequenceFeature : patientCADDscores.keySet())
		{
			MannWhitneyUTest m = new MannWhitneyUTest();
			double[] pats = convertDoubles(patientCADDscores.get(sequenceFeature));
			double[] pop = convertDoubles(populationCADDscores.get(sequenceFeature));
			double pval = m.mannWhitneyUTest(pats, pop);
	
			Mean mean = new Mean();
			Double lod = -Math.log10(pval);
			
			System.out.println("for " + sequenceFeature + " we have " +
		patientCADDscores.get(sequenceFeature).size() + " patient observations vs. " +
					populationCADDscores.get(sequenceFeature).size() + " population observations, lod = " + lod + ", meanPats = " + mean.evaluate(pats) + ", mean pop = " + mean.evaluate(pop));
			
			
			
		}
		
		
//		for(String sequenceFeature : patientCADDscores.keySet())
//		{
//			//print stuff to check in R
//			StringBuffer pats = new StringBuffer();
//			pats.append(sequenceFeature + "_pats\t");
//			for(Double cadd : patientCADDscores.get(sequenceFeature))
//			{
//				pats.append(cadd + "\t");
//			}
//			pats.deleteCharAt(pats.length()-1);
//			pats.append(")");
//			pw.println(pats.toString());
//			
//			StringBuffer pop = new StringBuffer();
//			pop.append(sequenceFeature + "_pop\t");
//			for(Double cadd : populationCADDscores.get(sequenceFeature))
//			{
//				pop.append(cadd + "\t");
//			}
//			pop.deleteCharAt(pop.length()-1);
//			pop.append(")");
//			pw.println(pop.toString());
//			
//			pw.flush();
//		}
//		
//		
//		
		pw.flush();
		pw.close();

	}
	
	/**
	 * LE SIGH.... java.... ffs.
	 * @param doubles
	 * @return
	 */
	public static double[] convertDoubles(List<Double> doubles)
	{
	    double[] ret = new double[doubles.size()];
	    for (int i = 0; i < ret.length; i++)
	    {
	        ret[i] = doubles.get(i).doubleValue();
	    }
	    return ret;
	}
	
	// TODO
	// wish i could reuse annotator code here
	private Double getCADDScoreForVariant(String chr, String pos, String alt)
	{

		return null;
	}

	private HashMap<String, ArrayList<Double>> calculatePopulationGenotypeCADDscores(
			HashMap<String, ArrayList<String>> populationExomeCandidateVariants, String inheritance) throws Exception
	{

		TabixReader caddTabixReader = new TabixReader(molgenisSettings.getProperty(CaddAnnotator.CADD_FILE_LOCATION_PROPERTY));
		
		HashMap<String, ArrayList<Double>> res = new HashMap<String, ArrayList<Double>>();

		int count = 0;
		for (String sequenceFeature : populationExomeCandidateVariants.keySet())
		{
			count++;
			System.out.println(count + " populationExomeCandidateVariants: " + sequenceFeature + " has "
					+ populationExomeCandidateVariants.get(sequenceFeature).size());

			if (!res.containsKey(sequenceFeature))
			{
				ArrayList<Double> caddScores = new ArrayList<Double>();
				res.put(sequenceFeature, caddScores);
			}

			for (String record : populationExomeCandidateVariants.get(sequenceFeature))
			{

				String[] recordSplit = record.split("\t", -1);
				String[] altSplit = recordSplit[4].split(",");

				if (altSplit.length > 1)
				{
					System.out.println("not supporting multi allelic alt variants at the moment.....");
					continue;
				}
				
				String pos = recordSplit[1];
				String ref = recordSplit[3];
				String alt = altSplit[0];

				// System.out.println("CADD: " + record.getString("CADD"));
				// System.out.println("EXAC: " + record.getString("EXAC_AF"));


				System.out.println("getting genotypes for " + record);

				// TODO: filter on quality??
				String infoSplit = recordSplit[7];

				
				// get het and homalt counts
				// "AC_Het=326;AC_Hom=2;"
				
				//TODO whats the difference between AN and AN_Adj ???
				
				String AC_Het = null;
				String AC_Hom = null;
				String AN_Adj = null;
				for (String infoPart : infoSplit.split(";", -1))
				{
					if (infoPart.startsWith("AC_Het"))
					{
						AC_Het = infoPart.replace("AC_Het=", "");
					}
					else if (infoPart.startsWith("AC_Hom"))
					{
						AC_Hom = infoPart.replace("AC_Hom=", "");
					}
					else if (infoPart.startsWith("AN_Adj"))
					{
						AN_Adj = infoPart.replace("AN_Adj=", "");
					}
				}

				if (AC_Het == null || AC_Hom == null)
				{
					throw new Exception("AC_Het == null || AC_Hom == null for " + record);
				}

				int iAC_Het = Integer.parseInt(AC_Het);
				int iAC_Hom = Integer.parseInt(AC_Hom);
				int iAN_Adj = Integer.parseInt(AN_Adj);
				

				System.out.println("iAC_Het = " + iAC_Het + ", iAC_Hom = " + iAC_Hom);
				
				Double caddScore = null;
				
				
				TabixReader.Iterator it = caddTabixReader.query(recordSplit[0] + ":" + recordSplit[1] + "-" + recordSplit[1]);
				String next;
				boolean resultFound = false;
				while (it != null && (next = it.next()) != null)
				{
				//	System.out.println("from cadd :" + next);
					String[] split = next.split("\t", -1);
					
					// TODO multi allelic

					// tabix can be fuzzy on the true positions... check it first
					if (!split[1].equals(pos))
					{
						continue;
					}

					caddScore = Double.parseDouble(split[4]);
					
					System.out.println("from cadd :" + caddScore);
				}

		
				//TODO correct ??
				int numberOfHomRef = ( iAN_Adj - iAC_Het - (2*iAC_Hom)) / 2;
				
				for(int het = 0; het < iAC_Het; het ++)
				{
					res.get(sequenceFeature).add(0.0);
					res.get(sequenceFeature).add(caddScore);
				}
				for(int homalt = 0; homalt < iAC_Hom; homalt ++)
				{
					res.get(sequenceFeature).add(caddScore);
					res.get(sequenceFeature).add(caddScore);
				}
				for(int homref = 0; homref < iAN_Adj; homref ++)
				{
					
					res.get(sequenceFeature).add(0.0);
					res.get(sequenceFeature).add(0.0);
				}
				
				

			}

		}

		return res;
	}

	/**
	 * Always returns a Double[] with 2 values, or NULL
	 * 
	 * @param CADD
	 * @param genotype
	 * @param inheritance
	 * @return
	 * @throws Exception
	 */
	private double[] getGenoCADDscore(String CADDstring, String genotype, String inheritance) throws Exception
	{
		if (genotype.equals("./."))
		{
			// missing genotype... skip
			return null;
		}

		double CADD = 999999999;
		try
		{
			CADD = Double.parseDouble(CADDstring);
		}
		catch (Exception e)
		{
			// System.out.println("failed to parse CADD score '" + CADDstring + "'..");
			// FIXME: happens a lot when using specialized CADD files e.g. 1000G or so
			return null;
		}

		String[] genoSplit = genotype.split("/", -1);
		if (genoSplit.length != 2)
		{
			throw new Exception("Genotype not splittable: " + genotype);
		}

		if (!(genoSplit[0].equals("0") || genoSplit[0].equals("1")))
		{
			throw new Exception("allele" + genoSplit[0] + " is not 0 or 1, others currently not supported");
		}

		if (!(genoSplit[1].equals("0") || genoSplit[1].equals("1")))
		{
			throw new Exception("allele" + genoSplit[1] + " is not 0 or 1, others currently not supported");
		}

		int allele0 = Integer.parseInt(genoSplit[0]);
		int allele1 = Integer.parseInt(genoSplit[1]);

		if (allele0 + allele1 == 0)
		{
			return new double[]
			{ 0.0, 0.0 };
		}

		else if (allele0 + allele1 == 2)
		{
			return new double[]
			{ CADD, CADD };
		}

		else if (allele0 + allele1 == 1)
		{
			if (inheritance.equals("dominant"))
			{
				return new double[]
				{ CADD, CADD };
			}
			else if (inheritance.equals("recessive"))
			{
				return new double[]
				{ 0.0, 0.0 };
			}
			else if (inheritance.equals("additive"))
			{
				return new double[]
				{ CADD, 0.0 };
			}
			else
			{
				throw new Exception("Inheritance " + inheritance + " unknown");
			}
		}

		else
		{
			throw new Exception("allele0 + allele1 unknown value " + (allele0 + allele1));
		}

	}

	/**
	 * Get lists of cadd scores per sequence feature (across all patients)
	 * 
	 * @param variantsPerGene
	 * @param inheritance
	 * @return
	 * @throws Exception
	 */
	private HashMap<String, ArrayList<Double>> calculatePatientGenotypeCADDscores(
			HashMap<String, ArrayList<Entity>> variantsPerGene, ArrayList<String> sampleIds, String inheritance)
			throws Exception
	{
		HashMap<String, ArrayList<Double>> res = new HashMap<String, ArrayList<Double>>();

		for (String gene : variantsPerGene.keySet())
		{
			System.out.println("calculatePatientGenotypeCADDscores: " + gene + " has "
					+ variantsPerGene.get(gene).size());

			if (!res.containsKey(gene))
			{
				ArrayList<Double> caddScores = new ArrayList<Double>();
				res.put(gene, caddScores);
			}

			for (Entity record : variantsPerGene.get(gene))
			{

				System.out.println("chr: " + record.getString("#CHROM") + ", pos:" + record.getString("POS")
						+ ", ref: " + record.getString("REF") + ", alt: " + record.getString("ALT") + ", cadd: "
						+ record.getString("CADD"));

				// StringBuffer sb = new StringBuffer();
				// sb.append("patient genotypes:");
				Iterable<Entity> sampleEntities = record.getEntities(VcfRepository.SAMPLES);
				for (Entity sample : sampleEntities)
				{

					// TODO known ugly....
					// NAME='905957_T_100386'
					// also: allows for sampleIds to be NULL in case its not supplied? TODO
					if (sampleIds != null && sampleIds.contains(sample.get("NAME").toString().split("_")[2]))
					{

						// sb.append(" " + sample.get("GT"));

						double[] genoCadd = getGenoCADDscore(record.getString("CADD"), sample.get("GT").toString(),
								inheritance);

						if (genoCadd != null)
						{

							res.get(gene).add(genoCadd[0]);
							res.get(gene).add(genoCadd[1]);

							// sb.append(genoCadd[0] + " " + genoCadd[1]);
						}
					}

				}
				// sb.append("\n");

				// System.out.println(sb.toString());

			}

			System.out.println("gene " + gene + " has a total of " + res.get(gene).size()
					+ " cadd scores for patient genotypes");

		}

		return res;
	}

	/**
	 * Get patient candidate variants. Map of SequenceFeature->Variants. Filtered on MAF and protein impact MODERATE or
	 * HIGH as annotated by snpEff.
	 * 
	 * @return
	 */
	public HashMap<String, ArrayList<Entity>> getPatientExomeCandidateVariants(VcfRepository patientVcfFile,
			double MAFthreshold)
	{
		HashMap<String, ArrayList<Entity>> res = new HashMap<String, ArrayList<Entity>>();

		Map<String, RepositoryAnnotator> annotators = applicationContext.getBeansOfType(RepositoryAnnotator.class);
		RepositoryAnnotator exacAnnotator = annotators.get("exac");
		RepositoryAnnotator caddAnnotator = annotators.get("cadd");

		// FIXME: well this sucks........ >_<
		// here we tell the VcfRepo that we want to annotate with ExAC and CADD
		// failing to do so will (a) cause any new attributes to be 'invisible'
		// and (b) each subsequent annotator to overwrite the previously added attributes
		DefaultEntityMetaData emd = (DefaultEntityMetaData) patientVcfFile.getEntityMetaData();
		DefaultAttributeMetaData infoAttribute = (DefaultAttributeMetaData) emd.getAttribute(VcfRepository.INFO);
		for (AttributeMetaData attribute : exacAnnotator.getOutputMetaData())
		{
			for (AttributeMetaData atomicAttribute : attribute.getAttributeParts())
			{
				infoAttribute.addAttributePart(atomicAttribute);
			}
		}
		for (AttributeMetaData attribute : caddAnnotator.getOutputMetaData())
		{
			for (AttributeMetaData atomicAttribute : attribute.getAttributeParts())
			{
				infoAttribute.addAttributePart(atomicAttribute);
			}
		}

		Iterator<Entity> vcfIterWithExac = exacAnnotator.annotate(patientVcfFile);
		Iterator<Entity> vcfIterWithExacAndCadd = caddAnnotator.annotate(vcfIterWithExac);

		while (vcfIterWithExacAndCadd.hasNext())
		{
			Entity record = vcfIterWithExacAndCadd.next();
			DefaultEntityMetaData newMeta = (DefaultEntityMetaData) record.getEntityMetaData();
			// newMeta.addAttribute(name)

			// System.out.println(record.toString());
			// System.out.println(record.getEntityMetaData().toString());

			String ref = record.getString("REF");
			String allAlts = record.getString("ALT");

			String[] altSplit = allAlts.split(",", -1);

			if (altSplit.length > 1)
			{
				System.out.println("WARNING: for now, ignoring multi allelic variants!! " + record.toString());
				continue;
			}

			for (int i = 0; i < altSplit.length; i++)
			{

				// System.out.println("CADD: " + record.getString("CADD"));
				// System.out.println("EXAC: " + record.getString("EXAC_AF"));

				String alt = altSplit[i];
				// System.out.println(i + "," + ref + ", " + alt);

				String[] annSplit = record.getString("INFO_ANN").split("\\|", -1);
				SnpEffServiceAnnotator.Impact impact = Enum.valueOf(SnpEffServiceAnnotator.Impact.class, annSplit[2]);

				String gene = annSplit[3];

				String exacMafString = record.getString(ExacAnnotator.EXAC_AF);
				Double exacMAF = 0.0;
				if (exacMafString != null && !exacMafString.isEmpty())
				{
					exacMAF = Double.parseDouble(exacMafString);
				}
				// System.out.println("getPatientExomeCandidateVariants exacMAF" + exacMAF);

				String filter = record.getString("FILTER");
				if (!filter.equals("PASS"))
				{
					continue;
				}

				if (exacMAF <= MAFthreshold
						&& (impact.equals(SnpEffServiceAnnotator.Impact.MODERATE) || impact
								.equals(SnpEffServiceAnnotator.Impact.HIGH)))
				{
					// System.out.println(gene + ", " + impact + ", " + exacMAF + ",");
				}

				if (res.containsKey(gene))
				{
					res.get(gene).add(record);
				}
				else
				{
					ArrayList<Entity> variants = new ArrayList<Entity>();
					variants.add(record);
					res.put(gene, variants);
				}

			}

			if (res.size() == 10)
			{
				System.out.println("we have 10 genes, quitting for dev purposes...");
				break;
			}
			else
			{
				// System.out.println("nr of genes: " + res.size());
			}

		}

		return res;
	}

	/**
	 * Get comparable population variants. Map of SequenceFeature->Variants. TODO: balance out genotypes and get more if
	 * there are none Use original ExAC lines to keep it simple
	 * 
	 * @throws Exception
	 */
	public HashMap<String, ArrayList<String>> getPopulationExomeCandidateVariants(
			HashMap<String, ArrayList<Entity>> candidates, File exacFileLocation, double MAF) throws Exception
	{
		HashMap<String, ArrayList<String>> res = new HashMap<String, ArrayList<String>>();

		TabixReader vcfTabixReader = new TabixReader(exacFileLocation.getAbsolutePath());

		for (String sequenceFeature : candidates.keySet())
		{
			// System.out.println("getPopulationExomeCandidateVariants SEQUENCEFEATURE " + sequenceFeature);
			ArrayList<String> popVars = new ArrayList<String>();
			res.put(sequenceFeature, popVars);

			// first we check how many patient variants were in ExAC and use those genotypes
			for (Entity patientVar : candidates.get(sequenceFeature))
			{
				// System.out.println("getPopulationExomeCandidateVariants patientVar" + patientVar);

				if (patientVar.getString("EXAC_AF") != null && !patientVar.getString("EXAC_AF").isEmpty())
				{
					System.out.println(sequenceFeature + " YES: " + patientVar.getString("EXAC_AF"));

					// get genotypes from ExAC
					TabixReader.Iterator it = vcfTabixReader.query(patientVar.getString("#CHROM") + ":"
							+ patientVar.getString("POS") + "-" + patientVar.getString("POS"));
					String next;

					boolean resultFound = false;

					while (it != null && (next = it.next()) != null)
					{
						String[] split = next.split("\t", -1);

						// tabix can be fuzzy on the true positions... check it first
						if (!split[1].equals(patientVar.getString("POS")))
						{
							continue;
						}

						// iterate over alt alleles
						String[] altSplit = split[4].split(",", -1);
						for (int i = 0; i < altSplit.length; i++)
						{

							if (split[3].equals(patientVar.getString("REF"))
									&& altSplit[i].equals(patientVar.getString("ALT")))
							{
								// TODO: directly get the appropriate MAF for this alt allele here??
								res.get(sequenceFeature).add(next);
								resultFound = true;
							}

						}

					}

					if (!resultFound)
					{
						throw new Exception("exac-patient variant not found back in exac!! " + patientVar);
					}

				}
				else
				{
					System.out.println(sequenceFeature + " NOPE !");
				}

			}

			// then we should "balance it out" by adding e.g. singleton variants from ExAC
			// ideally, the distribution of impact types and allele frequency is equal to the patient group

		}

		return res;
	}

	/**
	 * Calculate LOD scores per sequence feature. Map of SequenceFeature->LOD.
	 * 
	 * @return
	 */
	public HashMap<String, Double> scoreSequenceFeatures(HashMap<String, List<Entity>> candidates,
			HashMap<String, List<Entity>> population)
	{
		HashMap<String, Double> res = new HashMap<String, Double>();

		return res;
	}

}
